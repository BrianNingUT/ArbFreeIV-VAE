{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Description\n",
    "\n",
    "This notebook is used to fit the CTMC-VAE model once the CTMC model parameters have been prefitted to the IV data. Slight modifications can be made to fit other models shown on the manuscript (such as the L'evy processes). \n",
    "\n",
    "This notebook allows the exploration of certain hyperparameters, specifically beta, network structure, latent dimension, and batch size and the resultant networks are outputted to a separate folder in the Networks folder. For details of the full algorthim, please refer to our manuscript \"Arbitrage-Free Implied Volatility Surface Generation1with Variational Autoencoders\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ctmc\n",
    "from VAE_fit import fit_VAE\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy as dc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the json encoder to save network parameters to be loaded when loading each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in the fitted parameters from the appropriate data file and normalize the data to prepare for VAE training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 'AUD'\n",
    "batch_output_dir = 'Networks/' + ID + \"_CTMC_VAE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads training and testing days\n",
    "with open(\"all_cur_train_valid_days_new.pickle\", 'rb') as handle:\n",
    "    all_days = pickle.load(handle)\n",
    "with open('Data/' + ID + '_fitted_params.pickle', 'rb') as handle:\n",
    "    params = pickle.load(handle)\n",
    "    \n",
    "# Creates normalized data\n",
    "train_params = []\n",
    "ts = np.array(params['keys'])\n",
    "params = params['values']\n",
    "for day in all_days['train']:\n",
    "    train_params.append(torch.squeeze(params[np.where(ts==pd.Timestamp(day[:10]))]))\n",
    "train_params = torch.vstack(train_params)\n",
    "train_mean = torch.mean(train_params, dim=0)\n",
    "train_std = torch.std(train_params, dim=0)\n",
    "train_norm = (train_params - train_mean)/train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the hyperparameters we wish to explore over, the ones provided here are simply some examples that can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 2000    # Number of epochs before stopping\n",
    "\n",
    "param_dict ={    # Hyperparameters to run batch\n",
    "    'batch_size' : [200],\n",
    "    'latent_size' : np.array([3, 5, 10, 15]),\n",
    "    'hidden_dim' : np.array([64]),\n",
    "    'layers' : np.array([4]),\n",
    "    'beta' : np.array([0.01, 0.1, 1, 10]),\n",
    "}\n",
    "\n",
    "tau = np.array([0.08333333, 0.16666667, 0.25, 0.5, 0.75, 1., 3., 5.])\n",
    "\n",
    "param_mesh = np.array(np.meshgrid(*(list(range(len(param_dict[v]))) for i, v in enumerate(sorted(param_dict.keys())))), dtype = int).T.reshape(-1,len(param_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_idx(dict_keys, key):\n",
    "    key_list = sorted(dict_keys)\n",
    "    return key_list.index(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run the VAE training procedure and save the resultant network in a separate folder for each distinct set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_net(idx):\n",
    "    print(\"Starting run: \" + str(idx))\n",
    "    # create new directory\n",
    "    net_output_dir = batch_output_dir + \"network\" + str(idx) + \"/\"\n",
    "    Path(net_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # construct hidden_dims\n",
    "    hidden_dims = []\n",
    "    for i in range(param_dict['layers'][param_mesh[idx, dict_idx(param_dict.keys(), 'layers')]]):\n",
    "        hidden_dims.append(param_dict['hidden_dim'][param_mesh[idx, dict_idx(param_dict.keys(), 'hidden_dim')]] * (2 ** i))\n",
    "    hidden_dims.reverse()\n",
    "        \n",
    "    # fit network\n",
    "    losses, vae = fit_VAE(\n",
    "        epochs=epochs,\n",
    "        lr=0.001,\n",
    "        full_data=train_norm.clone().detach(),\n",
    "        batch_size=param_dict['batch_size'][param_mesh[idx, dict_idx(param_dict.keys(), 'batch_size')]],\n",
    "        weight_decay=1e-3,\n",
    "        latent_dim=param_dict['latent_size'][param_mesh[idx, dict_idx(param_dict.keys(), 'latent_size')]],\n",
    "        beta=param_dict['beta'][param_mesh[idx,dict_idx(param_dict.keys(), 'beta')]],\n",
    "        hidden_dims=dc(hidden_dims),\n",
    "    )\n",
    "    \n",
    "    # Save each part of the network\n",
    "    torch.save(vae.encoder.state_dict(), net_output_dir + \"encoder\")\n",
    "    torch.save(vae.fc_mu.state_dict(), net_output_dir + \"fc_mu\")\n",
    "    torch.save(vae.fc_var.state_dict(), net_output_dir + \"fc_var\")\n",
    "    torch.save(vae.decoder_input.state_dict(), net_output_dir + \"decoder_input\")\n",
    "    torch.save(vae.decoder.state_dict(), net_output_dir + \"decoder\")\n",
    "    torch.save(vae.final_layer.state_dict(), net_output_dir + \"final_layer\")\n",
    "    \n",
    "    # Define parameter dictionary and write to text file using json.\n",
    "    sum_dict = {\n",
    "        'batch_size' : param_dict['batch_size'][param_mesh[idx, dict_idx(param_dict.keys(), 'batch_size')]],\n",
    "        'latent_size' : param_dict['latent_size'][param_mesh[idx, dict_idx(param_dict.keys(), 'latent_size')]],\n",
    "        'hidden_dim' : hidden_dims,\n",
    "        'beta':param_dict['beta'][param_mesh[idx, dict_idx(param_dict.keys(), 'beta')]],\n",
    "    }\n",
    "    with open(net_output_dir + 'param_sum.txt', 'w') as file:\n",
    "        file.write(json.dumps(sum_dict, cls=NpEncoder)) \n",
    "    \n",
    "    # Save the normalized training data, normalization mean and variance to reference file inside the network folder\n",
    "    data_sum = {}\n",
    "    data_sum['train_norm'] = train_norm\n",
    "    data_sum['train_std'] = train_std\n",
    "    data_sum['train_mean'] = train_mean\n",
    "    with open(net_output_dir + 'data_sum.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_sum, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: 1\n",
      "Starting run: 2\n",
      "Starting run: 3\n",
      "Starting run: 4\n",
      "Starting run: 5\n",
      "Starting run: 6\n",
      "Starting run: 7\n",
      "Starting run: 8\n",
      "Starting run: 9\n",
      "Starting run: 10\n",
      "Starting run: 11\n",
      "Starting run: 12\n",
      "Starting run: 13\n",
      "Starting run: 14\n",
      "Starting run: 15\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,16):\n",
    "    run_single_net(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
